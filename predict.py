import os.path
import cv2
from tools.log_mode import LogManager
from argparse import ArgumentParser
import time
import platform
import psutil
import torch
import yaml
from tqdm import tqdm
from typing import Dict, List

from ultralytics import SAM, __version__ as ultralytics_version
from ultralytics.models.sam import SAM2VideoPredictor, SAM2DynamicInteractivePredictor

class SAMPredictor:
    # æ˜ç¡®å£°æ˜åˆå§‹åŒ–éœ€è¦çš„å‚æ•°
    REQUIRED_INIT_PARAMS = ["model_path"]
    '''
    sam url: https://docs.ultralytics.com/zh/models/sam-2/#segment-with-prompts
    '''
    def __init__(self, model_path: str,
                 log_manager: LogManager = None,
                 mode: str = "image",
                 ):
        self.log_manager = log_manager

        self.mode =  mode
        load_model_start = time.time()
        self.log_manager.debug("ğŸš€ å¼€å§‹åŠ è½½SAMæ¨¡å‹...")
        if mode in ["image", "video2img"]:
            self.log_manager.debug(f"ğŸš€ æ¨¡å¼ä¸º{mode}ï¼Œå¤„ç†å›¾ç‰‡æ¨¡å¼...")
            self.model = SAM(model_path)
        elif mode == "video":
            self.log_manager.debug(f"ğŸš€ æ¨¡å¼ä¸º{mode}ï¼Œmodelä¸º{model_path}ï¼Œå¤„ç†è§†é¢‘æ¨¡å¼...")
            overrides = dict(conf=0.25, task="segment", mode="predict", imgsz=1024, model=model_path)
            self.model = SAM2VideoPredictor(overrides=overrides)
            # overrides = dict(conf=0.25, task="segment", mode="predict", imgsz=1024, model="sam2_b.pt")
            # predictor = SAM2VideoPredictor(overrides=overrides)
        elif mode == "DynVideo":
            self.log_manager.debug(f"ğŸš€ æ¨¡å¼ä¸º{mode}ï¼Œå¤„ç†åŠ¨æ€äº¤äº’æ¨¡å¼...")
            log_manager.info(f"model_path: {model_path}")
            # overrides = dict(conf=0.01, task="segment", mode="predict", imgsz=1024, model=model_path, save=False)
            # self.model = SAM2DynamicInteractivePredictor(overrides)
            overrides = dict(conf=0.01, task="segment", mode="predict", imgsz=1024, model="sam2_t.pt", save=False)
            self.model = SAM2DynamicInteractivePredictor(overrides=overrides, max_obj_num=10)

        load_model_elapsed = time.time() - load_model_start
        self.log_manager.debug(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ | è€—æ—¶: {load_model_elapsed:.2f}s")


    def __call__(self, input_data,
                 bboxes: List[List[int]] = None,
                 points: List[List[int]] = None,
                 labels: List[int] = None):
        self.log_manager.debug(f"ğŸš€ æ¨¡å‹å¼€å§‹å¤„ç†...")
        self.log_manager.debug(f"ğŸš€ input_data: {input_data}")
        self.log_manager.debug(f"ğŸš€ bboxes: {bboxes}")
        self.log_manager.debug(f"ğŸš€ points: {points}")
        self.log_manager.debug(f"ğŸš€ labels: {labels}")
        return self.model(input_data, points=points, labels=labels)


    def __del__(self):
        self.log_manager.info("âœ… æ¨¡å‹å·²é‡Šæ”¾")


def get_system_info() -> Dict[str, str]:
    """é‡‡é›†ç³»ç»Ÿ/ç¡¬ä»¶æ ¸å¿ƒä¿¡æ¯ï¼ˆç”¨äºæ—¥å¿—è¾“å‡ºï¼‰"""
    # CPUä¿¡æ¯
    cpu_info = {
        "å‹å·": platform.processor() or "æœªçŸ¥CPU",
        "æ ¸å¿ƒæ•°": f"{psutil.cpu_count(logical=True)} (é€»è¾‘) / {psutil.cpu_count(logical=False)} (ç‰©ç†)",
        "ä½¿ç”¨ç‡": f"{psutil.cpu_percent(interval=0.1)}%"
    }

    # GPUä¿¡æ¯ï¼ˆåŸºäºPyTorchï¼‰
    gpu_info = {}
    if torch.cuda.is_available():
        gpu_info["æ•°é‡"] = torch.cuda.device_count()
        gpu_info["å‹å·"] = torch.cuda.get_device_name(0)
        gpu_info["CUDAç‰ˆæœ¬"] = torch.version.cuda
        gpu_info["æ˜¾å­˜"] = f"{torch.cuda.get_device_properties(0).total_memory / 1024 ** 3:.1f}GB"
    else:
        gpu_info["çŠ¶æ€"] = "æ— å¯ç”¨GPU / ä½¿ç”¨CPUæ¨ç†"

    # å†…å­˜ä¿¡æ¯
    mem = psutil.virtual_memory()
    mem_info = {
        "æ€»å†…å­˜": f"{mem.total / 1024 ** 3:.1f}GB",
        "å¯ç”¨å†…å­˜": f"{mem.available / 1024 ** 3:.1f}GB",
        "ä½¿ç”¨ç‡": f"{mem.percent}%"
    }

    # åŸºç¡€ç¯å¢ƒä¿¡æ¯
    basic_info = {
        "Pythonç‰ˆæœ¬": platform.python_version(),
        "æ“ä½œç³»ç»Ÿ": f"{platform.system()} {platform.release()}",
        "Ultralyticsç‰ˆæœ¬": ultralytics_version,
        "PyTorchç‰ˆæœ¬": torch.__version__,
        "æ¨ç†è®¾å¤‡": "GPU" if torch.cuda.is_available() else "CPU"
    }

    return {
        "=== ç³»ç»Ÿç¯å¢ƒä¿¡æ¯ ===": "",
        **basic_info,
        "\n=== CPUä¿¡æ¯ ===": "",
        **cpu_info,
        "\n=== GPUä¿¡æ¯ ===": "",
        **gpu_info,
        "\n=== å†…å­˜ä¿¡æ¯ ===": "",
        **mem_info
    }


def print_system_info():
    """æ ¼å¼åŒ–æ‰“å°ç³»ç»Ÿä¿¡æ¯åˆ°æ—¥å¿—"""
    sys_info = get_system_info()
    print("=" * 80)
    print("ğŸ“‹ ç³»ç»Ÿ/ç¡¬ä»¶ç¯å¢ƒä¿¡æ¯")
    print("=" * 80)
    for key, value in sys_info.items():
        if "===" in key:
            print()
            print(f"\033[36m{key}\033[0m")  # è“è‰²é«˜äº®åˆ†éš”ç¬¦ï¼ˆå¯é€‰ï¼‰
        else:
            print(f"  {key:<10}: {value}")
    print("=" * 80)


def format_inference_args(args_dict: Dict[str, any]) -> str:
    """æ ¼å¼åŒ–æ¨ç†å‚æ•°ï¼ˆç”¨äºæ—¥å¿—è¾“å‡ºï¼‰"""
    formatted = []
    for k, v in args_dict.items():
        if isinstance(v, list):
            formatted.append(f"{k}={[round(x, 2) if isinstance(x, float) else x for x in v]}")
        else:
            formatted.append(f"{k}={v}")
    return " | ".join(formatted)


def parse_args():
    """å®Œå–„å‚æ•°è§£æï¼ˆè¡¥å……ç¼ºå¤±çš„model_pathï¼‰"""
    parser = ArgumentParser(description="SAM (Segment Anything Model) æ¨ç†è„šæœ¬")
    parser.add_argument("--config", type=str, default="./config/sam_2.yaml",
                        help="é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤: config.yaml")
    parser.add_argument("--log_level", type=str, default="DEBUG",
                        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
                        help="æ—¥å¿—çº§åˆ«ï¼Œå¯é€‰: DEBUG/INFO/WARNING/ERRORï¼Œé»˜è®¤: INFO")
    parser.add_argument("--mode", type=str, default="video2img",
                        choices=["img", "video2img", "video", "DynVideo"],
                        help="é¢„å¤„ç†æ¨¡å¼ï¼Œå¯é€‰: é»˜è®¤ä¸ºç©º")
    args = parser.parse_args()
    return args


if __name__ == "__main__":
    # 1. åˆå§‹åŒ–æ—¥å¿—ç®¡ç†å™¨
    args = parse_args()
    assert os.path.exists(args.config), f"é…ç½®æ–‡ä»¶ {args.config} ä¸å­˜åœ¨"

    # è¯»å–é…ç½®æ–‡ä»¶
    with open(args.config, "r") as f:
        config = yaml.safe_load(f)

    assert os.path.exists(config["input_data_path"]), f"å›¾ç‰‡è·¯å¾„ {config['input_data_path']} ä¸å­˜åœ¨"
    os.makedirs(config["output_data_path"], exist_ok=True)
    assert os.path.exists(config["output_data_path"]), f"è¾“å‡ºè·¯å¾„ {config['output_data_path']} ä¸å­˜åœ¨"

    log_manager = LogManager()
    log_manager.set_log_level(args.log_level)

    # åˆå§‹åŒ–é¢„æµ‹å™¨ï¼ˆä»…ä¼ å…¥æ‰€éœ€å‚æ•°ï¼‰
    sam_predictor = SAMPredictor(model_path=config["model_path"], mode=args.mode, log_manager=log_manager,)

    # 2. æ‰“å°ç³»ç»Ÿä¿¡æ¯ï¼ˆæ—¥å¿—å¤´éƒ¨ï¼‰
    print_system_info()

    # 3. æ‰“å°è§£æåçš„å‚æ•°
    log_manager.info("=" * 80)
    log_manager.info("âš™ï¸  argså‚æ•°")
    log_manager.info("=" * 80)
    for arg in vars(args):
        log_manager.info(f"  {arg:<12}: {getattr(args, arg)}")
    log_manager.info("=" * 80)
    log_manager.info("âš™ï¸  configå‚æ•°")
    log_manager.info("=" * 80)
    for key, value in config.items():
        log_manager.info(f"  {key:<12}: {value} type: {type(value)}")
    log_manager.info("=" * 80)

    # 4. æ¨¡å‹æ¨ç†
    log_manager.info("ğŸš€ å¼€å§‹æ¨ç†...")
    results = sam_predictor(config["input_data_path"], bboxes=config["bboxes"], points=config["points"], labels=config["labels"])
    log_manager.info(f"âœ… æ¨ç†å®Œæˆï¼Œå…±å¤„ç† {len(results)} ä¸ªæ ·æœ¬")

    if len(results) == 1:
        results[0].save(config["output_data_path"])
    elif len(results) > 1:
        log_manager.warning(f"âš ï¸  å¤šä¸ªç»“æœï¼Œè¯·è‡ªè¡Œå¤„ç†ï¼Œ é€‰æ‹©æ¨¡å¼ä¸º {args.mode}")
    else:
        log_manager.error("âŒ æœªå¤„ç†ä»»ä½•æ ·æœ¬")
